
---
title: "IODS final project"
author:
  name: Riku Laine
  affiliation: riku.laine[at]helsinki.fi
output:
  html_document:
    theme: yeti
    toc: true
    toc_depth: 3
    fig_caption: true
    code_folding: hide
---

<center> <h1>PROJECT TITLE</h1> </center>

```{r Settings, include=FALSE}
library(knitr)
library(ggplot2)
library(GGally)
library(corrplot)
library(dplyr)
opts_chunk$set(message = F)
opts_chunk$set(warning = F)
opts_chunk$set(out.width = "100%")

mycor <- function(data, mapping, alignPercent = 0.6, method = "pearson", 
    use = "complete.obs", corAlignPercent = NULL, corMethod = NULL, 
    corUse = NULL, sgnf=3, ...) {
    if (!is.null(corAlignPercent)) {
        stop("'corAlignPercent' is deprecated.  Please use argument 'alignPercent'")
    }
    if (!is.null(corMethod)) {
        stop("'corMethod' is deprecated.  Please use argument 'method'")
    }
    if (!is.null(corUse)) {
        stop("'corUse' is deprecated.  Please use argument 'use'")
    }
    useOptions <- c("all.obs", "complete.obs", "pairwise.complete.obs", 
        "everything", "na.or.complete")
    use <- pmatch(use, useOptions)
    if (is.na(use)) {
        warning("correlation 'use' not found.  Using default value of 'all.obs'")
        use <- useOptions[1]
    } else {
        use <- useOptions[use]
    }
    cor_fn <- function(x, y) {
        cor(x, y, method = method, use = use)
    }
    xCol <- deparse(mapping$x)
    yCol <- deparse(mapping$y)
    if (GGally:::is_date(data[[xCol]]) || GGally:::is_date(data[[yCol]])) {
        if (!identical(class(data), "data.frame")) {
            data <- fix_data(data)
        }
        for (col in c(xCol, yCol)) {
            if (GGally:::is_date(data[[col]])) {
                data[[col]] <- as.numeric(data[[col]])
            }
        }
    }
    if (is.numeric(GGally:::eval_data_col(data, mapping$colour))) {
        stop("ggally_cor: mapping color column must be categorical, not numeric")
    }
    colorCol <- deparse(mapping$colour)
    singleColorCol <- ifelse(is.null(colorCol), NULL, paste(colorCol, 
        collapse = ""))
    if (use %in% c("complete.obs", "pairwise.complete.obs", "na.or.complete")) {
        if (length(colorCol) > 0) {
            if (singleColorCol %in% colnames(data)) {
                rows <- complete.cases(data[c(xCol, yCol, colorCol)])
            } else {
                rows <- complete.cases(data[c(xCol, yCol)])
            }
        } else {
            rows <- complete.cases(data[c(xCol, yCol)])
        }
        if (any(!rows)) {
            total <- sum(!rows)
            if (total > 1) {
                warning("Removed ", total, " rows containing missing values")
            } else if (total == 1) {
                warning("Removing 1 row that contained a missing value")
            }
        }
        data <- data[rows, ]
    }
    xVal <- data[[xCol]]
    yVal <- data[[yCol]]
    if (length(names(mapping)) > 0) {
        for (i in length(names(mapping)):1) {
            tmp_map_val <- deparse(mapping[names(mapping)[i]][[1]])
            if (tmp_map_val[length(tmp_map_val)] %in% colnames(data)) 
                mapping[[names(mapping)[i]]] <- NULL
            if (length(names(mapping)) < 1) {
                mapping <- NULL
                break
            }
        }
    }
    if (length(colorCol) < 1) {
        colorCol <- "ggally_NO_EXIST"
    }
    if ((singleColorCol != "ggally_NO_EXIST") && (singleColorCol %in% 
        colnames(data))) {
        cord <- plyr::ddply(data, c(colorCol), function(x) {
            cor_fn(x[[xCol]], x[[yCol]])
        })
        colnames(cord)[2] <- "ggally_cor"
        cord$ggally_cor <- signif(as.numeric(cord$ggally_cor), 
            sgnf)
        lev <- levels(data[[colorCol]])
        ord <- rep(-1, nrow(cord))
        for (i in 1:nrow(cord)) {
            for (j in seq_along(lev)) {
                if (identical(as.character(cord[i, colorCol]), 
                  as.character(lev[j]))) {
                  ord[i] <- j
                }
            }
        }
        cord <- cord[order(ord[ord >= 0]), ]
        cord$label <- GGally:::str_c(cord[[colorCol]], ": ", cord$ggally_cor)
        xmin <- min(xVal, na.rm = TRUE)
        xmax <- max(xVal, na.rm = TRUE)
        xrange <- c(xmin - 0.01 * (xmax - xmin), xmax + 0.01 * 
            (xmax - xmin))
        ymin <- min(yVal, na.rm = TRUE)
        ymax <- max(yVal, na.rm = TRUE)
        yrange <- c(ymin - 0.01 * (ymax - ymin), ymax + 0.01 * 
            (ymax - ymin))
        p <- ggally_text(label = GGally:::str_c("Corr: ", signif(cor_fn(xVal, 
            yVal), sgnf)), mapping = mapping, xP = 0.5, yP = 0.9, 
            xrange = xrange, yrange = yrange, color = "black", 
            ...) + theme(legend.position = "none")
        xPos <- rep(alignPercent, nrow(cord)) * diff(xrange) + 
            min(xrange, na.rm = TRUE)
        yPos <- seq(from = 0.9, to = 0.2, length.out = nrow(cord) + 
            1)
        yPos <- yPos * diff(yrange) + min(yrange, na.rm = TRUE)
        yPos <- yPos[-1]
        cordf <- data.frame(xPos = xPos, yPos = yPos, labelp = cord$label)
        cordf$labelp <- factor(cordf$labelp, levels = cordf$labelp)
        p <- p + geom_text(data = cordf, aes(x = xPos, y = yPos, 
            label = labelp, color = labelp), hjust = 1, ...)
        p
    }  else {
        xmin <- min(xVal, na.rm = TRUE)
        xmax <- max(xVal, na.rm = TRUE)
        xrange <- c(xmin - 0.01 * (xmax - xmin), xmax + 0.01 * 
            (xmax - xmin))
        ymin <- min(yVal, na.rm = TRUE)
        ymax <- max(yVal, na.rm = TRUE)
        yrange <- c(ymin - 0.01 * (ymax - ymin), ymax + 0.01 * 
            (ymax - ymin))
        p <- ggally_text(label = paste("Corr:\n", signif(cor_fn(xVal, 
            yVal), sgnf), sep = "", collapse = ""), mapping, xP = 0.5, 
            yP = 0.5, xrange = xrange, yrange = yrange, ...) + 
            theme(legend.position = "none")
        p
    }
}

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Abstract

Some stuff $$y = \alpha x_1 + \beta x_2 + \epsilon$$, which happens to be regression function for linear regression.

## Research question and hypothesis

I was interested in finding out what factors of people's learning are the most connected to their final score. Could boundness in the course syllabus be somehow in connection with a low score? I concluded that logistic regression was the most fitting tool to answer this question.

I hypothesized that along with attitude `RelatingIdeas` (relating ideas to one's previous knowledge and experience) would affect the final score as it represents how much does a student ponder the things taught in their free time. And when one revises things learned it deepens their learning, as "repetitio est mater studiorum".

## Data description

[The data](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt) was acquired from a study made by Kimmo Vehkalahti. The study included a questionnaire which was held in Fall 2014 for the participants of an introductory course to statistics (Johdatus yhteiskuntatilastotieteeseen) and it was funded by the Teachers' Academy. The questionnaire itself had questions regarding for instance the students' motivation, strategies in studying and general interest in statistics.

The variables -- apart from gender and age -- describe the following attributes of an individual (synthesized based on data from [here](http://www.psy.gla.ac.uk/~steve/dands/dandstable.html)^1^ and [here](http://www.etl.tla.ed.ac.uk/questionnaires/scoringkey.pdf)^2^:

* `Attitude` -- Respondent's general attitude towards studying statistics
* `SeekingMeaning` -- Intention to understand for oneself^2^
* `RelatingIdeas` -- relating ideas (things learned) to one's previous knowledge and experience^2^
* `UseOfEvidence` -- Examining the logic of arguments^1^
* `LackOfPurpose` -- Not reflecting on purpose or strategies^2^
* `UnrelatedMemorizing` -- Memorising facts and procedures routinely^2^
* `SyllabusBoundness` --  Concentrating only on what is required for assessment^2^
* `OrganizedStudying` -- Preparing for classes and how organized is in studies
* `TimeManagement` -- How good at managing ones time.


### Data processing

The original data set consisted of 183 observations, but after [processing it](https://raw.githubusercontent.com/Riku-Laine/IODS-final/master/data/createLearningFP.R) only 166 observations remained due to the exclusion of individuals who had zero points from the final exam from not attending it. Processing the data, I constructed the variables for e.g. use of evidence and seeking meaning by the instructions in the [meta page](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt) and named them as above. For instance, `SeekingMeaning` is the mean for the answers D03, D11, D19 and D27 which were answered in the 5-point [Likert scale](https://en.wikipedia.org/wiki/Likert_scale). For the logistic regression, variable `HighPoints` is constructed to be true if the person received more points than the median of the group(23). 

### The data visually

First I explored the data with a paired plot matrix to see possible trends and correlations between the variables of the data. Afterwards, I deepened my analysis with some variables of interest revealed by the plot matrix.

```{r Plots}
learning2014 <- read.csv("~/GitHub/IODS-final/data/learning2014.csv")

ggpairs(learning2014[,-13], mapping = aes(col=Gender, alpha=0.1), lower = list(combo = wrap("facethist", bins = 20)),  upper = list(continuous = wrap(size = 1.7, mycor, sgnf=2)))+
	theme(strip.text = element_text(size=6),
				strip.text.y = element_text(angle=0),
				axis.text = element_text(size=6),
				strip.text.x = element_text(angle=45))+
	ggtitle("Figure 1: Interactions in Learning2014 data")
```

Distributions of the variables seem to be quite symmetrical, although most of the respondents are students and therefore it is not surprising that the inter-quartile range of age falls between 21 and 27 years. There are only minor differences between the genders, males seem to have a better attitude towards statistics as females appear to be more organized in their studies. Males' better attitude towards mathematics have been shown in previous studies.

From figure 1 above it is visible that there are some quite strong correlations with some of the variables. For instance the variables of deep learning `OrganizedStudying` and `TimeManagement` have a correlation coefficient of $\rho \approx 0,7$ as the attitude of a student correlates with their points with a coefficient $\rho \approx 0,43$. Correlations are analyzed more thoroughly below figure 2.

```{r Corrplot, out.height="80%", out.width="80%"}
corrplot(cor(learning2014[, -c(1, 13)]), type = "lower", diag = T, tl.col='black', tl.srt = 0.1, tl.offset = 1, 
				 title = "Fig. 2: Correlations of learning2014 data")
```

From figure 2 it can be asserted that our variables correlate the most with the variables they comprise the original variables (such as Deep_adj) with. Also notable is the correlation with the students' attitude and points from the final exam being approximately `r round(cor(learning2014$Points, learning2014$Attitude), digits=2)`. This means better attitude associates with better performance in the exam. There are practically no other correlations that could cause collinearity in our regression model as all the others have an absolute value approximately smaller than 0.3.

### Gender differences in level of organization in studying and attitude toward statistics

In figure 1 there were some gender differences noted. Let's look briefly into them. First, let us construct box plots of those variables and do a quick statistical test to assert the observed differences.

```{r plotteja, fig.asp=.4}
p1 <- ggplot(learning2014, aes(x=Gender, y=Attitude, fill=Gender))+
	geom_boxplot()+
	geom_jitter(position=position_jitter(0.1), alpha=.4)+ggtitle("Gender differences", subtitle = "Fig. 3 Attitude")

p2 <- ggplot(learning2014, aes(x=Gender, y=OrganizedStudying, fill=Gender))+
	geom_boxplot()+
	geom_jitter(position=position_jitter(0.1), alpha=.4)+ggtitle("", subtitle = "Fig. 4 OrganizedStudying")

multiplot(p1, p2, cols=2)
```

It can be seen that men have higher attitude points, which is supported by Mann-Whitney U-test ($p \approx 1.1 \cdot 10^{-4}$). Females seem to be more organized (Mann-Whitney U, $p \approx 7.1 \cdot 10^{-3}$) in a statistically significant quantity. These differences have been noted in previous studies.

## Methods

### Logistic regression

As [NCBI](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065119/) puts it, logistic regression "is a method for modelling the dependence of a binary response variable on one or more explanatory variables." In our case we have the binary response variable of points gotten in the final exam; if one has a score higher than the median of points (23) they will have 1, as ones with a lower score get a zero. The regression model then attempts to estimate the dependent variable in the best fashion. The prediction $\textbf{Y}$ will be constructed as a linear combination of our explanatory values, such that a minimum  amount of residual is created.

The model offers the impact of the explanatory variables as log odds ratios. By exponentiating them, the achieved "odds ratio can be used to quantify the relationship between X and Y. Odds higher than 1 mean that X is positively associated with 'success'" as [Tuomo explained](https://tuomonieminen.github.io/Helsinki-Open-Data-Science/#/30).

### Other statistical tests used

#### Mann-Whitney U-test

Mann-Whitney U-test is a non-parametric alternative for the t-test. In other words, we do not have the assume that our variables are normally distributed. The test allows us to test "the null hypothesis that it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample" ([Wikipedia](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)). Fig. 5 shows that the distributions are quite normal in `Attitude` but it has been shown that Mann-Whitney doesn't lose its statistical power compared to the t-test in such a quantity it would be beneficial to risk on the normality assumption.

```{r, fig.align='center', out.height="50%"}
plot(density(learning2014$Attitude[learning2014$Gender=='M']), col='blue', main="Figure 5: Distribution of Attitude by gender", xlab="Attitude points")
lines(density(learning2014$Attitude[learning2014$Gender=='F']), col='red')
legend("topright", col=c("red", "blue"), legend = c('Female', 'Male'), lty=1)
```


## Results

First I constructed logistic regression model over all the variables in the data to predict `HighScores`. Summary of the model can be seen below.

```{r }
model <- glm(HighPoints ~ Gender + Age + Attitude + SeekingMeaning + RelatingIdeas + UseOfEvidence + LackOfPurpose + UnrelatedMemorising + SyllabusBoundness + OrganizedStudying + TimeManagement, data = learning2014, family = "binomial")

summary(model)
```

From the above summary it can be concluded that only the students attitude towards statistics and time managing skills matter in regard of performance in the final exam. Residuals of the model are somewhat symmetrically distributed. In table 1 below are the regression coefficients, 95% confidence intervals and p-values for our covariates. From there it is seen, that for every attitude point gained, the probability of success gets significantly greater (OR 1.15, 95% CI from 1.08 to 1.22, $p \approx 1.70\cdot 10^{-5}$). Also time management is playing a key role in determining final exam scores. Its OR estimate is approximately 2,3, but it was scaled from 1 to 5 as attitude ranged from 14 to 50.

```{r, results='asis'}
tableLogReg <- cbind(Coefficient = coef(model), confint(model)) %>% exp %>% round(., digits = 4)
tableLogReg <- cbind(tableLogReg, `P-value` = round(summary(model)$coefficients[, 4], digits = 4))
kable(tableLogReg, caption = "Table 1: Regression coefficients, 95% confidence intervals and p-values")
```

```{r}
p3 <- ggplot(learning2014, aes(x=HighPoints, y=Attitude))+
	geom_boxplot()+ylim(10,50)+
	geom_jitter(position=position_jitter(0.1), alpha=.4)+ggtitle("", subtitle = "Fig. 6 HighPoints and Attitude")

p4 <- ggplot(learning2014, aes(x=HighPoints, y=TimeManagement))+
	geom_boxplot()+
	geom_jitter(position=position_jitter(0.1), alpha=.4)+ggtitle("", subtitle = "Fig. 7 HighPoints and TimeManagement")

multiplot(p3, p4, cols=2)
```

As negative results it can be noted that for instance gender and age had no effect on scoring. Connecting ideas to the previously learned or experienced might be an interesting variable to analyze in the following studies. It seem quite odd how it seems to be (in a non-significant matter) in contact with poorer performance in final exam as the OR estimate with its intervals are close to be under 1.

### Validation analysis

For validation analysis, I will perform likelihood ratio tests to see if a reduced model would be more suitable to model the exam scores and in addition i will cross tabulate the predicted values of our model to the actual to see the predictive power of it.

Initially our model was called in form `HighPoints ~ Gender + Age + Attitude + SeekingMeaning + RelatingIdeas + UseOfEvidence + LackOfPurpose + UnrelatedMemorising + SyllabusBoundness +  OrganizedStudying + TimeManagement`, but in the summary of the formula it was seen that some of the arguments, especially Gender, Age and SeekingMeaning weren't at all significant so I removed them from the model and constructed `model2` which I proceeded to test against the first model with a likelihood ratio test. As seen below, the test returned non-significant ($p > 0.05$) so those variables were non-significant, although it wouldn't effect the output of the model. After brief exploration it in the sense of likelihood ratio test there was no significant differences in between model with all variables and the model with only `Attitude`, `RelatingIdeas` and `TimeManagement`.

```{r}
model2 <- glm(HighPoints ~ Attitude + RelatingIdeas+ TimeManagement, data = learning2014, family = "binomial")
anova(model, model2, test = "LRT")
anova(model, test = "LRT")
summary(model2)
```

Below is also a similar validation analysis which was done in DataCamp. First we predicted the values of HighPoints and the cross tabulated them with the actual values. So it was concluded that our model predicted success approximately 73% right.  There were a total of 45 miss-predictions, but partly they might have arisen from the fact that there were 13 students with the median score 23. all in all there were 34 students in one-point radius from the median (meaning [22,24]).

```{r}
# predict the probability
probabilities <- predict(model, type = "response")

learning2014  <- mutate(learning2014, probability = probabilities)

learning2014  <- mutate(learning2014, prediction = probability > 0.5)

# tabulating the target variable versus the predictions
table(HighPoints = learning2014$HighPoints, Prediction = learning2014$prediction)
```

## Conclusions and discussion

Form this it an be concluded that 'paejb
PBNAT'P'GRJ
PEorjg'petjg'paejt'tgja